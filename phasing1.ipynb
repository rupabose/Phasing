{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import constraints\n",
    "import functools\n",
    "from functools import partial\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import time\n",
    "import concurrent.futures\n",
    "import gzip\n",
    "import logging\n",
    "import subprocess\n",
    "from subprocess import check_output\n",
    "from subprocess import check_call\n",
    "import sys\n",
    "from sys import argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_ref_file(filename, header_position): #for 1kg files, header=95\n",
    "    df=pd.read_csv(filename, header=header_position, sep='\\t')\n",
    "    del df['#CHROM']\n",
    "    dropcol=df.columns[1:8] #removes the non POS columns before the sample columns with the genotype calls\n",
    "    df=df.drop(dropcol, axis=1)\n",
    "    #at this point we have a VCF file in the dataframe with only the samples and calls, and the position\n",
    "    return df\n",
    "\n",
    "def ref_call_positions_bp(filename, header_position): #store the positions and remove them from the data frame\n",
    "    df=make_ref_file(filename, header_position)\n",
    "    positions_bp=df['POS']\n",
    "    return positions_bp\n",
    "\n",
    "def cleaned_df(filename, header_position):\n",
    "    df = make_ref_file(filename, header_position)\n",
    "    del df['POS']\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_ref_file(filename):  # if using a cleaned up ref file as made in make_ref_file\n",
    "    df = pd.read_csv(filename, sep=\"\\t\")\n",
    "    df_arrays = np.array(df)\n",
    "    correct_df = df_arrays.transpose()\n",
    "    indiv_names = df.columns\n",
    "    clean_input = np.insert(correct_df, 0, indiv_names, axis=1)\n",
    "    return correct_df\n",
    "def individual_names(filename):\n",
    "    df = pd.read_csv(filename, sep=\"\\t\")\n",
    "    indiv_names = df.columns\n",
    "    return indiv_names\n",
    "\n",
    "def get_names(df): #if already read in df; for use in recursive calls\n",
    "    return df.columns\n",
    "\n",
    "def file_input(filename):\n",
    "    correct_df = read_in_ref_file(filename)\n",
    "    indiv_names = individual_names(filename)\n",
    "    clean_input = np.insert(correct_df, 0, indiv_names, axis=1)\n",
    "    return clean_input\n",
    "#result of this is the cleaned up samples, with the correct sample names for each array of genotype calls\n",
    "def positions_bp(filename):\n",
    "    pass\n",
    "def reference_input(filename):\n",
    "    reference_file = read_in_ref_file(filename)\n",
    "    reference_positions = positions_bp(filename)  # not defined yet#\n",
    "    ref_names = individual_names(filename)\n",
    "    newarrayshape = reference_file.shape\n",
    "    cleaned_input_ref = np.zeros(list(newarrayshape) + [2])\n",
    "    for i, element in enumerate(reference_file):\n",
    "        for j, w in enumerate(element):\n",
    "            cleaned_input_ref[i][j] = [int(call) for call in w.split(',')]\n",
    "    return cleaned_input_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_in_ref_file(\"~/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class split_test_train():\n",
    "    \"\"\"\n",
    "    splits the reference file (phasing truth set) input into test and train sets\n",
    "    uses 20% for testing, 80% for training\n",
    "    \"\"\"\n",
    "    def decision_tree(train, test): \n",
    "        #train is the % of the dataset to use for training\n",
    "        #test is the % of the dataset to use for testing\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class samples_from_file():\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    -file including all samples to be phased\n",
    "\n",
    "    OUTPUTS:\n",
    "    -dataframe with all samples' calls\n",
    "    -all base pair positions with genotype calls for the samples\n",
    "    \"\"\"\n",
    "    def make_sample_file(filename, header_position):\n",
    "        df = pd.read_csv(filename, header=header_position, sep='\\t')\n",
    "        # assuming VCF #this may need to change depending on file\n",
    "        del df['#CHROM']\n",
    "        sample_positions_bp = df['POS']\n",
    "        # removes the non POS columns before the sample columns with the genotype calls\n",
    "        dropcol = newdf.columns[1:8]\n",
    "        df = df.drop(dropcol, axis=1)\n",
    "\n",
    "        self.get_sample_POS = sample_positions_bp\n",
    "        #at this point we have a VCF file in the dataframe with only the samples and calls, and the position\n",
    "        return df, sample_positions_bp\n",
    "\n",
    "    def sample_indiv(sample_file):\n",
    "        sample_df = clean_inputs(sample_file)\n",
    "        return sample_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class windows_phasing():\n",
    "    def __init__(self, chrom, window_size, reference_panel, samples_input):\n",
    "        self.chrom = chrom\n",
    "        self.window_size = window_size\n",
    "        self.reference_panel = reference_panel\n",
    "        self.samples_input = samples_input\n",
    "\n",
    "    def windowing(self):\n",
    "        \"\"\"\n",
    "        \"INPUT:\"\n",
    "        -hapmap recombination rate file (averaged for all populations)\n",
    "            -gives cMs for windowing and base pair positions to use to organize into windows\n",
    "        -base pair positions of calls for individuals in the reference panel\n",
    "\n",
    "        OUPTUT:\n",
    "        RETURNS the window ranges in terms of cM and bp positions \n",
    "        input is the chromosome number and the desired window size\n",
    "        *can build this into a variable size or sliding window definition\n",
    "        \"\"\"\n",
    "        filename_pattern = '~/testpy/rupasandbox/hapmap_recombination_rate_hg38/hapmap_recomb_hg38_chr{}.txt'\n",
    "        self.recomb_hapmap = pd.read_csv(\n",
    "            filename_pattern.format(self.chrom), sep=\" \")\n",
    "        x1 = 0\n",
    "        x2 = 25\n",
    "        window_cM = window_size\n",
    "        listcM = [x*10 for x in range(x1, x2)]\n",
    "        windowpoints = []\n",
    "        windowpoints_bp = []\n",
    "        for element in listcM:\n",
    "            for i in range(len(self.recomb_hapmap)):\n",
    "                if self.recomb_hapmap['Genetic_Map(cM)'][i] > element and self.recomb_hapmap['Genetic_Map(cM)'][i-1] < element:\n",
    "                    windowpoints.append(\n",
    "                        self.recomb_hapmap['Genetic_Map(cM)'][i])\n",
    "                    windowpoints_bp.append(self.recomb_hapmap['position'][i])\n",
    "        windowpoints.append(\n",
    "            self.recomb_hapmap['Genetic_Map(cM)'][len(self.recomb_hapmap)-1])\n",
    "        windowpoints_bp.append(\n",
    "            self.recomb_hapmap['position'][len(self.recomb_hapmap)-1])\n",
    "\n",
    "        windows = []\n",
    "        windows_bp = []\n",
    "        for i in range(len(windowpoints)-1):\n",
    "            newpoint = (windowpoints[i], windowpoints[i+1])\n",
    "            newbppoint = (windowpoints_bp[i], windowpoints_bp[i+1])\n",
    "            windows.append(newpoint)\n",
    "            windows_bp.append(newbppoint)\n",
    "        self.windows = windows\n",
    "        self.windows_bp = windows_bp\n",
    "\n",
    "    # we are searching within the positions that indicate the ()cM window\n",
    "    def window_search(self):\n",
    "        \"\"\"\n",
    "        INPUTS: \n",
    "        the windows_bp defines the BP positions which bound each ()cM window\n",
    "        the reference_samples = the reference panel \n",
    "\n",
    "        what it does: \n",
    "        searches through the reference panel within a window and returns the homozygosity pattern for each individual\n",
    "        if the homozygosity pattern has already been found and logged, then it just adds the individual indicator for the signal\n",
    "            the individual indicator is the first positin in the array\n",
    "\n",
    "        we don't have the positions stored in the same array as the genotype calls, so we are actually pulling this from a different set\n",
    "        in the array with the genotype calls, we actually start from index 1 (index 0 is the sample identifier)\n",
    "        \"\"\"\n",
    "        pop_subsets = {}\n",
    "        for (windowstart, windowend) in self.windows_bp:  # iterating through each window\n",
    "            for individual in self.reference_samples:  # going individual by individual in the reference panel\n",
    "                # so we can iterate through the genotype calls of each individual easily\n",
    "                individual_ref = self.reference_samples[individual]\n",
    "                individual_window_ref = individual_ref[windowstart:windowend]\n",
    "                homozyg_sig = ''  # starting a new homozygous signature as a string, which represents the homozyg pattern of this individual\n",
    "                for g1, g2 in individual_window_ref:\n",
    "                    if g1 == g2:  # true if it is homozygous (e.g. 0,0 or 1,1)\n",
    "                        # we store position and the call, 0 or 1 for ref/alt\n",
    "                        homozyg_sig += f\"{individual_ref['POS']}|{g1}\"\n",
    "                # if this specific homozyg pattern in this window is not already represented, we add it\n",
    "                if pop_subsets.get(homozyg_sig) is None:\n",
    "                    # we also add the individual under it\n",
    "                    pop_subsets[homozyg_sig] = [individual]\n",
    "                else:\n",
    "                    # if this is already represented in the dict, then we just store that the individual has it too\n",
    "                    pop_subsets[homozyg_sig].append(individual)\n",
    "        return pop_subsets\n",
    "\n",
    "    def windowing_samples_on_reference_panel():\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class phasing_search_model():\n",
    "    def __init__(self, reference_panel, chr, unknown_samples):\n",
    "        self.reference_panel = reference_panel\n",
    "        self.chr = chr\n",
    "        self.unknown_samples = unknown_samples\n",
    "\n",
    "    def windowing_on_panel(self):\n",
    "        windows_phasing.windowing(self.unknown_samples)\n",
    "\n",
    "    def window_matching_samples_to_panel():\n",
    "        pass\n",
    "\n",
    "    def match_find_v2(self):\n",
    "        for homozygous_sig in pop_subsets:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\"\n",
    "The first run through will be to phase everythign with obvious homozyg sig -->  1 haplotype longest match\n",
    "Then all further iterations will move through updating longest runs with each successive iteration\n",
    "--these will be defined separately.\n",
    "\n",
    "\n",
    "Singly linked list — Traversal of items can be done in the forward direction only.\n",
    "Doubly linked list — Traversal of items can be done in both forward and backward directions. Nodes consist of an additional pointer known as prev, pointing to the previous node.\n",
    "Circular linked lists — Linked lists where the prev pointer of the head\n",
    "\"\"\"\n",
    "#written like the first runs post-burn-in of shapeit\n",
    "\n",
    "\n",
    "class first_run():\n",
    "    def __init__(self, sequences, reference_panel, chr):\n",
    "        self.sequences = sequences\n",
    "        self.reference_panel = reference_panel\n",
    "        self.chr = chr\n",
    "\n",
    "    def search_first_run(self):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "        -reference panel that is sorted into numpy hash table structure = pop_subsets\n",
    "        -samples that are sorted into same hash table structure or 3d array structure \n",
    "            by homozygous signature\n",
    "        -frequency counts for each sig based on the reference panel and the freq for each allele\n",
    "\n",
    "        OUTPUT:\n",
    "        -state(homozygous_signature_subset, node_start, haplotype, count)\n",
    "            this output feeds into the extend_haplotype function and allows\n",
    "            us to continue phasing in the next iteration\n",
    "        -last_marker in matching haplotype\n",
    "    \n",
    "        \"\"\"\n",
    "    #map applies a function to an iterable map(function,iterable e.g. list)\n",
    "\n",
    "    def extend_haplotype(self):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "        - state{homozygous_signature_subset,node_start,haplotype,count}: \n",
    "            current state information\n",
    "        - next_marker: (0 or 1) with which to attempt extension of haplotype\n",
    "        OUTPUT:\n",
    "        - if successful extension, update state and return SUCCESS\n",
    "        - else, return FAILURE\n",
    "            -lossy/error\n",
    "                -extend one more out, if failure, then deem failure\n",
    "                -extend one more out, if success, then store and also consider\n",
    "                    longest next match if updated to failure and repeated at new hap\n",
    "            the output is fed back in to extend or change haplotypes and continue\n",
    "            phasing in further iterations\n",
    "        \"\"\"\n",
    "\n",
    "    def update_states(self):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "        \n",
    "        OUTPUT:\n",
    "\n",
    "        \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
